{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1069/3703165931.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Visualization\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Preprocessing and Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, f_classif\n",
    "\n",
    "# Model and Estimators\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Model Selection and Validation\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, classification_report, roc_curve, log_loss\n",
    "\n",
    "# Imbalanced Dataset Handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy.stats import uniform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'classifier__n_neighbors': 11, 'classifier__weights': 'distance', 'preprocessor__num__feature_selection__k': 10}\n",
      "ROC AUC Score: 0.7032812279960569\n",
      "Accuracy: 0.8333333333333334\n",
      "Confusion Matrix: \n",
      "[[670 119]\n",
      " [ 17  10]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91       789\n",
      "           1       0.08      0.37      0.13        27\n",
      "\n",
      "    accuracy                           0.83       816\n",
      "   macro avg       0.53      0.61      0.52       816\n",
      "weighted avg       0.95      0.83      0.88       816\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "data = pd.read_csv(\"NHANES_data_train.csv\")\n",
    "\n",
    "# Specify numeric and categorical columns\n",
    "numeric_columns = ['Income', 'Age', 'Diastolic', 'Systolic', 'Pulse', 'BMI', 'HDL', 'Trig', 'LDL', 'TCHOL', 'kidneys_eGFR']\n",
    "categorical_columns = ['Sex', 'Race', 'Edu', 'Diabetes', 'CurrentSmoker', 'isActive', 'isInsured']\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer()),  \n",
    "    ('scaler', StandardScaler()),  \n",
    "    ('feature_selection', SelectKBest(f_classif, k=10))  \n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_columns),\n",
    "    ('cat', categorical_pipeline, categorical_columns)\n",
    "])\n",
    "\n",
    "# Define a pipeline for resampling and classification\n",
    "model_pipeline = ImPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('resampling', SMOTE(random_state=42)), \n",
    "    ('classifier', KNeighborsClassifier())  \n",
    "])\n",
    "\n",
    "def remap_labels(target):\n",
    "    return 0 if target == 2 else target\n",
    "\n",
    "targets = data['MI'].apply(remap_labels)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop(['MI'], axis=1), targets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'classifier__n_neighbors': [11],  \n",
    "    'classifier__weights': ['distance'],  \n",
    "    'preprocessor__num__feature_selection__k': [10]  \n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "clf = GridSearchCV(model_pipeline, param_grid, scoring='roc_auc', cv=5)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_clf = clf.best_estimator_\n",
    "\n",
    "# Fit the calibrated classifier on the validation data\n",
    "X_train_calib, X_val_calib, y_train_calib, y_val_calib = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "calibrated_clf = CalibratedClassifierCV(best_clf, cv='prefit')\n",
    "calibrated_clf.fit(X_val_calib, y_val_calib)\n",
    "\n",
    "y_prob = calibrated_clf.predict_proba(X_test)[:, 1]\n",
    "y_pred = calibrated_clf.predict(X_test)  # Predicting the classes\n",
    "\n",
    "# Output evaluation metrics\n",
    "print(f\"Best Parameters: {clf.best_params_}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_prob)}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"Classification Report: \\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant ID and MI probabilities saved to kNN_pred.csv.\n"
     ]
    }
   ],
   "source": [
    "# Apply the trained model to new data\n",
    "new_data = pd.read_csv(\"NHANES_test_data_4_students.csv\")\n",
    "if 'MI' in new_data.columns:\n",
    "    new_data.drop(columns=['MI'], inplace=True)\n",
    "\n",
    "# Predict probabilities on the new dataset\n",
    "new_probabilities = calibrated_clf.predict_proba(new_data.drop(['ParticipantID'], axis=1))[:, 1]\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ParticipantID': new_data['ParticipantID'],\n",
    "    'MI_Probability': new_probabilities\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv(\"kNN_pred.csv\", index=False)\n",
    "print(\"Participant ID and MI probabilities saved to kNN_pred.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script to create custom true_MI_labels.csv to be used in calculations for the Evaluator, provoded by Andy Markland\n",
      "     ParticipantID  MI\n",
      "0             3193   1\n",
      "1             4022   1\n",
      "2             1565   1\n",
      "3             2538   1\n",
      "4             1637   1\n",
      "..             ...  ..\n",
      "155           2462   0\n",
      "156           3001   0\n",
      "157           2619   0\n",
      "158           2692   0\n",
      "159           2785   0\n",
      "\n",
      "[160 rows x 2 columns]\n",
      "[0 1]\n",
      "Participant ID and MI probabilities saved to kNN_pred_proba_K.csv with true_MI_Label_kNN.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1069/1149103910.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trueLabels['MI'] = trueLabels['MI'].replace({2: 0, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"Script to create custom true_MI_labels.csv to be used in calculations for the Evaluator, provoded by Andy Markland\")\n",
    "dataset = pd.read_csv(\"NHANES_data_train.csv\")\n",
    "\n",
    "partition = dataset.sample(frac=0.5)\n",
    "\n",
    "group_1 = partition[partition['MI'] == 1]\n",
    "group_2 = partition[partition['MI'] == 2]\n",
    "\n",
    "count_group_1 = len(group_1)\n",
    "count_group_2 = len(group_2)\n",
    "\n",
    "if count_group_1 < count_group_2:\n",
    "    group_2_downsampled = group_2.sample(n=count_group_1)\n",
    "    balanced_dataset = pd.concat([group_1, group_2_downsampled])\n",
    "else:\n",
    "    group_1_downsampled = group_1.sample(n=count_group_2)\n",
    "    balanced_dataset = pd.concat([group_1_downsampled, group_2])\n",
    "\n",
    "balanced_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "new_data = balanced_dataset\n",
    "trueLabels =  new_data[['ParticipantID', 'MI']]\n",
    "trueLabels['MI'] = trueLabels['MI'].replace({2: 0, 1: 1})\n",
    "print(trueLabels)\n",
    "if 'MI' in new_data.columns:\n",
    "    new_data.drop(columns=['MI'], inplace=True)\n",
    "\n",
    "# Predict probabilities on the new dataset\n",
    "new_probabilities = calibrated_clf.predict_proba(new_data.drop(['ParticipantID'], axis=1))[:, 1]\n",
    "print(clf.classes_)\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ParticipantID': new_data['ParticipantID'],\n",
    "    'MI_Probability': new_probabilities\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv(\"kNN_pred_proba_KL.csv\", index=False)\n",
    "trueLabels.to_csv(\"true_MI_Label_kNN.csv\", index=False)\n",
    "trueLabels.to_csv(\"true_MI_Label_kNN.csv\", index=False)\n",
    "print(\"Participant ID and MI probabilities saved to kNN_pred_proba_K.csv with true_MI_Label_kNN.csv.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Takes a while but i could not get it to be any more efficent while being quick\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best Parameters: {'classifier__estimator__C': 0.21584494295802448, 'classifier__estimator__l1_ratio': 0.9699098521619943}\n",
      "ROC AUC Score: 0.7912970004224757\n",
      "Accuracy: 0.7610294117647058\n",
      "Confusion Matrix: \n",
      "[[603 186]\n",
      " [  9  18]]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.76      0.86       789\n",
      "           1       0.09      0.67      0.16        27\n",
      "\n",
      "    accuracy                           0.76       816\n",
      "   macro avg       0.54      0.72      0.51       816\n",
      "weighted avg       0.96      0.76      0.84       816\n",
      "\n",
      "Log Loss: 0.4969409124622444\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "data = pd.read_csv(\"NHANES_data_train.csv\")\n",
    "\n",
    "print(\"Takes a while but i could not get it to be any more efficent while being quick\")\n",
    "\n",
    "# Define preprocessing pipelines\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('imputer', KNNImputer()),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_columns),\n",
    "    ('cat', categorical_pipeline, categorical_columns)])\n",
    "\n",
    "# Adjust the logistic regression classifier for Elastic Net regularization\n",
    "logistic_regression = LogisticRegression(max_iter=10000, solver='saga', penalty='elasticnet', l1_ratio=0.5, random_state=42)\n",
    "\n",
    "# Define a pipeline for resampling, logistic regression, and calibration\n",
    "model_pipeline = ImPipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('resampling', SMOTE(random_state=42)),\n",
    "    ('classifier', CalibratedClassifierCV(estimator=logistic_regression, cv=5))  # Corrected parameter name\n",
    "])\n",
    "\n",
    "# Split the dataset\n",
    "X = data.drop(['MI'], axis=1)\n",
    "y = data['MI'].apply(remap_labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter distribution for hyperparameter tuning\n",
    "param_distributions = {\n",
    "    'classifier__estimator__C': uniform(0.01, 10),\n",
    "    'classifier__estimator__l1_ratio': uniform(0, 1)  # Appropriate for 'elasticnet' penalty\n",
    "}\n",
    "\n",
    "# Perform randomized search with cross-validation focusing on log loss\n",
    "clf = RandomizedSearchCV(model_pipeline, param_distributions, n_iter=10, scoring='neg_log_loss', cv=3, verbose=1, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation\n",
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Best Parameters: {clf.best_params_}\")\n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_prob)}\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "print(f\"Confusion Matrix: \\n{confusion_matrix(y_test, y_pred)}\")\n",
    "print(f\"Classification Report: \\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"Log Loss: {log_loss(y_test, y_prob)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Participant ID and MI probabilities saved to LogisticRegression_pred.csv.\n"
     ]
    }
   ],
   "source": [
    "# Apply the trained model to new data\n",
    "new_data = pd.read_csv(\"NHANES_test_data_4_students.csv\")\n",
    "if 'MI' in new_data.columns:\n",
    "    new_data.drop(columns=['MI'], inplace=True)\n",
    "\n",
    "# Predict probabilities on the new dataset\n",
    "new_probabilities = clf.predict_proba(new_data.drop(['ParticipantID'], axis=1))[:, 1]\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ParticipantID': new_data['ParticipantID'],\n",
    "    'MI_Probability': new_probabilities\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv(\"LogisticRegression_pred.csv\", index=False)\n",
    "print(\"Participant ID and MI probabilities saved to LogisticRegression_pred.csv.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script to create custom true_MI_labels.csv to be used in calculations for the Evaluator\n",
      "     ParticipantID  MI\n",
      "0             1913   1\n",
      "1              684   1\n",
      "2              842   1\n",
      "3              442   1\n",
      "4             2787   1\n",
      "..             ...  ..\n",
      "111           3489   0\n",
      "112            536   0\n",
      "113             61   0\n",
      "114           2839   0\n",
      "115           1472   0\n",
      "\n",
      "[116 rows x 2 columns]\n",
      "[0 1]\n",
      "Participant ID and MI probabilities saved to LogisticRegression_pred_proba_KL.csv with true_MI_Label_log.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1069/259683477.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  trueLabels['MI'] = trueLabels['MI'].replace({2: 0, 1: 1})\n"
     ]
    }
   ],
   "source": [
    "print(\"Script to create custom true_MI_labels.csv to be used in calculations for the Evaluator\")\n",
    "dataset = pd.read_csv(\"NHANES_data_train.csv\")\n",
    "\n",
    "partition = dataset.sample(frac=0.5)\n",
    "\n",
    "group_1 = partition[partition['MI'] == 1]\n",
    "group_2 = partition[partition['MI'] == 2]\n",
    "\n",
    "count_group_1 = len(group_1)\n",
    "count_group_2 = len(group_2)\n",
    "\n",
    "if count_group_1 < count_group_2:\n",
    "    group_2_downsampled = group_2.sample(n=count_group_1)\n",
    "    balanced_dataset = pd.concat([group_1, group_2_downsampled])\n",
    "else:\n",
    "    group_1_downsampled = group_1.sample(n=count_group_2)\n",
    "    balanced_dataset = pd.concat([group_1_downsampled, group_2])\n",
    "\n",
    "balanced_dataset.reset_index(drop=True, inplace=True)\n",
    "\n",
    "new_data = balanced_dataset\n",
    "trueLabels =  new_data[['ParticipantID', 'MI']]\n",
    "trueLabels['MI'] = trueLabels['MI'].replace({2: 0, 1: 1})\n",
    "print(trueLabels)\n",
    "if 'MI' in new_data.columns:\n",
    "    new_data.drop(columns=['MI'], inplace=True)\n",
    "\n",
    "# Predict probabilities on the new dataset\n",
    "new_probabilities = clf.predict_proba(new_data.drop(['ParticipantID'], axis=1))[:, 1]\n",
    "print(clf.classes_)\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'ParticipantID': new_data['ParticipantID'],\n",
    "    'MI_Probability': new_probabilities\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv(\"LogisticRegression_pred_proba_KL.csv\", index=False)\n",
    "trueLabels.to_csv(\"true_MI_Label_log.csv\", index=False)\n",
    "print(\"Participant ID and MI probabilities saved to LogisticRegression_pred_proba_KL.csv with true_MI_Label_log.csv.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
